{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51815d54",
   "metadata": {},
   "source": [
    "# Practical 3: EnKF in L96\n",
    "\n",
    "The objective of this practical is to perform DA experiments in the Lorenz-96 model with an ensemble Kalman\n",
    "filter testing out the impact of the type of Kalman filter, inflation and localisation on the DA performance.\n",
    "\n",
    "The differential equations of the Lorenz-96 model are given as\n",
    "\\begin{equation}\n",
    "\\frac{\\partial x_{n}}{\\partial t} = (x_{n+1}-x_{n-2})x_{n-1} - x_{i} + F\n",
    "\\end{equation}\n",
    "with $0 \\leq n < N_{x}$, and $x_{n}(t)$ assumed to be periodic, e.g. $x_{N_{x}}(t)=x_{0}(t)$. \n",
    "\n",
    "Let's start with importing all functions that will be used in this practical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e35ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tools.L96_model import lorenz96\n",
    "from tools.plots import plotL96, plot_LocMatrix, plotL96obs\n",
    "from tools.enkf_exp import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48bebb5",
   "metadata": {},
   "source": [
    "## Nature run\n",
    "\n",
    "This section generates the nature run of the experiment, i.e. what we consider to be\n",
    "the truth. You can change the initial condition `x0`, the final time `tmax` (consider that the model time\n",
    "step is 0.025 time units), the number of variables `Nx` and the forcing `F` in the model. For the benefit of speed\n",
    "and in order to display figures in an easier manner, we will use `Nx = 12`.\n",
    "\n",
    "This model can be run from any given initial condition, but the default is to spin it up from a\n",
    "perturbation around the unstable fixed point of the system $x_{n}(t)=F\\,\\forall n$. \n",
    "You will get a Hovmoller diagram (a contour plot showing the time evolution of the different variables \n",
    "in a circle of latitude), as well as a figure with $N_{x}$ panels. \n",
    "\n",
    "We also create a new initial condition that is different from the truth in `xguess`. \n",
    "In the end, we want data assimilation to bring the model run started off from this perturbed initial condition \n",
    "closer to the truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728cbbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'L96' #Model to be used. \n",
    "Nx = int(12) # number of state variables. Must be multiple of 4.\n",
    "F = 8 #Forcing\n",
    "x0 = np.array([0.,.05] + (Nx-2)*[0.]) + F # let it spin from rest (x_n(t=0) = F, forall n apart n=1)\n",
    "tmax = 8 # The final time of the nature run simulation\n",
    "discard = 20 #Spinup period to discard from output. \n",
    "dt = 0.025 #Time step integration scheme.\n",
    "\n",
    "#Create truth.\n",
    "t = np.arange(0, tmax+dt, dt)\n",
    "xt = lorenz96(x0, tmax, dt, discard, F)\n",
    "\n",
    "#Plot truth.\n",
    "plotL96(t, np.array(xt), Nx, model)\n",
    "\n",
    "#Initial conditions different from truth.\n",
    "np.random.seed(100)\n",
    "xguess = xt[:,0] + np.random.normal(size=np.shape(xt[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ccefc-3365-45b8-b597-df2d8aae2f70",
   "metadata": {},
   "source": [
    "## Data assimilation\n",
    "\n",
    "Next, we will create an experiment object. This object holds the settings and results for our Lorenze-96 ensemble Kalman filter\n",
    "experiments. An experiment using default settings can be created with `exp = Experiment(x0=x0, t=t, xt=xt)` where `x0` is the initial condition, `xt` is the truth model trajectory, `t` is the time coordinate. The forcing of the Lorenz-96 model can be modified by `exp = Experiment(x0=x0, t=t, xt=xt, F=forcing)`. After creation, \n",
    "the experiment can be run using `exp.run()` can be carried out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a71e09b-7060-4c6b-a069-58be6578b2cb",
   "metadata": {},
   "source": [
    "The default setup of each experiment has 24 ensemble members using serial ensemble Kalman filter. The default\n",
    "settings can be overwritten by passing them to the initialiser. E.g. `exp = Experiment(x0=x0, t=t, xt=xt, n_ens=12)` creates an\n",
    "experiment that uses the default settings for all parameters except the ensemble size `n_ens`. This has been changed\n",
    "from the default 24 members to 12 members."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e837380",
   "metadata": {},
   "source": [
    "In this experiment, we create \"artificial\" observations from the truth we just generated. This is done by applying the \n",
    "linear operator $\\mathbf{H}$ to the state $x(t)$.\n",
    "$\\mathbf{H}$ can take on many shapes, e.g. $\\mathbf{H}$ can observe all variables (`obs_grid='all'`),\n",
    "every other variables (`obs_grid='1010'`), the first 6 variables/half of the domain (`obs_grid='landsea'`), or six observations of a footprint (`obs_grid='foot_6', footprint=3`) . The observation options can be given when the experiment is initialised by `Experiment(..., obs_grid='1010)`.\n",
    "\n",
    "By default, the state is observed at every time step, but you can set the number of time steps between\n",
    "observations with the input argument `period_obs`.\n",
    "    \n",
    "Finally, we add some white random noise to our samples of the nature run. This noise represents the \n",
    "observational errors from e.g. instrument errors. The observational error covariance matrix $\\mathbf{R}$ is \n",
    "assumed to be diagonal (common assumption), but you can set the observational variance (i.e. the values on \n",
    "the diagonal of $\\mathbf{R}$) with `var_obs`.  The default value for `var_obs` is 2.\n",
    "\n",
    "In summary, the observations at time $t$, $y(t)$, are given as $y(t) = \\mathbf{H}x(t) + \\epsilon(t)$. Here\n",
    "$\\epsilon(t)$ is a random realisation from the normal distribution $\\mathcal{N}(0,\\mathbf{R})$. The settings\n",
    "to generate observations together with a plot of them are created in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the sake of creating observations, the initial conditon has to be the same as the truth\n",
    "exp = Experiment(x0=x0, t=t, xt=xt)\n",
    "exp.create_observations()\n",
    "\n",
    "#Plot the truth together with observations. \n",
    "plotL96obs(exp.tobs, exp.y, exp.n_obs, str(exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c707a8a6",
   "metadata": {},
   "source": [
    "## State estimation\n",
    "\n",
    "In this section we will set the observational standard deviation to\n",
    "$\\sigma_{obs}=\\sqrt{2}$, and look at the effect of varying the following: the observational frequency `period_obs`, \n",
    "the observation density `obs_grid`, the ensemble size `n_ens`. \n",
    "\n",
    "### Stochastic ensemble Kalman filter\n",
    "First, we will try to assimilate the observations using the default DA method, stochastic ensemble Kalman filter (`da_method='SEnKF'`). \n",
    "For reference, in the stochastic ensemble Kalman filter the analysis of the $n$th ensemble member, $x^{a,(n)}$, \n",
    "i.e. the ensemble member after the application of DA, is given by \n",
    "\n",
    "\\begin{equation}\n",
    "    x^{a,(n)} = x^{b,(n)} + \\mathbf{K}(y - \\mathbf{H}x^{b,(n)} + \\mathbf{R}^{\\frac{1}{2}}\\epsilon^{(n)})\n",
    "\\end{equation}\n",
    "\n",
    "Here $x^{b,(n)}$ is the model state in the $n$th ensemble member before DA at a time $t$, \n",
    "$\\epsilon$ a realisation from the normal distribution $\\mathcal{N}(0,\\mathbf{R})$ and \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{K} = \\mathbf{P}^{f} \\mathbf{H}^{\\rm{T}} (\\mathbf{H} \\mathbf{P}^{f} \\mathbf{H}^{\\rm{T}} + \\mathbf{R})^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "the Kalman gain matrix with background error covariance\n",
    "\\begin{equation}\n",
    "\\mathbf{P}^{f} = \\frac{1}{N_{ens}-1} \\sum_{n=1}^{N_{ens}} (x^{b,(n)}-\\overline{x^{b}}) \n",
    "(x^{b,(n)}-\\overline{x^{b}})^{\\rm{T}}\n",
    "\\end{equation}\n",
    "\n",
    "and $\\overline{x^{b}} = \\frac{1}{N_{ens}}\\sum_{n=1}^{N_{ens}} x^{b,(n)}$ the forecast ensemble mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03b64e",
   "metadata": {},
   "source": [
    "We rerun the model with the perturbed initial condition and assimilate all variables every 10 time steps. We plot the truth (black) together with the output just after the DA correction (purple). \n",
    "The 1st plot shows all ensemble members, the 2nd one only the ensemble mean. Finally, we also plot \n",
    "the spread (standard deviation) of the ensemble and the root-mean-squared error (RMSE). I.e. the\n",
    "RMS between the ensemble mean of the background/analysis and the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create experiment with a initial state that deviates from the truth. \n",
    "exp = Experiment(x0=xguess, t=t, xt=xt)\n",
    "\n",
    "#Run the experiment\n",
    "exp.run()\n",
    "\n",
    "#Plot model output\n",
    "exp.plot_state()\n",
    "\n",
    "#Plot metrics as function of every nstep.\n",
    "nstep = 1\n",
    "exp.plot_metrics(nstep)\n",
    "\n",
    "#Calculate and show as table the root-mean-square values over time. \n",
    "np.sqrt((exp.calculate_metrics(1)**2).mean(dim=['time'])).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30311b95",
   "metadata": {},
   "source": [
    "1. Rerun the aforementioned experiment but now add `period_obs=2` and plot the RMSE (hint: `exp = Experiment(x0=xguess, t=t, xt=xt, period_obs=2)`). \n",
    "Repeat with `period_obs=20`. How does observational frequency influence the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea0159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1a92af9",
   "metadata": {},
   "source": [
    "2. Try running the experiment with `x0=xguess, obs_grid='1010'` and `x0=xguess, obs_grid='landsea'`. \n",
    "How does the type of observations influence the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3f2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fdbbb59",
   "metadata": {},
   "source": [
    "3. Try running with an 6-member ensemble (`x0=xguess, n_ens=6`) and a 12-member ensemble (`x0=xguess, n_ens=16`). \n",
    "How does the ensemble size influence the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce03cb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62574c05",
   "metadata": {},
   "source": [
    "## Inflation\n",
    "\n",
    "Now let us try a more challenging setting by limiting our ensemble size from $N_{ens}=24$ to $N_{ens} = 12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create experiment with a initial state that deviates from the truth and with 12 ensemble members.\n",
    "exp = Experiment(x0=xguess, t=t, xt=xt, n_ens=12)\n",
    "\n",
    "#Run the experiment\n",
    "exp.run()\n",
    "\n",
    "#Plot the experiment's output. \n",
    "exp.plot_state()\n",
    "\n",
    "#Plot metrics\n",
    "exp.plot_metrics(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b1b0ef",
   "metadata": {},
   "source": [
    "The fundamental *ansatz* of ensemble Kalman\n",
    "filters is that the truth and each ensemble member are realisation of the same probability distribution. I.e. the\n",
    "truth can be viewed as another, but unknown, ensemble member. So, the standard deviation of the truth \n",
    "around the ensemble mean is equal to that of the ensemble members. The square-root of the former is \n",
    "the RMSE, that of the latter the ensemble spread. So, if the system is properly calibrated the expectation value\n",
    "$E[\\frac{{RMSE}^2}{\\sigma_{ens}^2}]=1$. \n",
    "\n",
    "Clearly, this is not the case in this experiment: the spread severely \n",
    "underestimates the RMSE after $t=2$. One way to solve this is to use inflation. Inflation increases the spread \n",
    "by rescaling the ensemble. The most common approach is to multiply the perturbations from the ensemble mean\n",
    "by a factor. In this practical, the an inflation factor of $\\alpha$ means that the $n$th ensemble member after inflation is\n",
    "given as $(1+\\alpha)(\\mathbf{x}^{b,(n)}-\\overline{\\mathbf{x}^{b}}) + \\overline{\\mathbf{x}^{b}}$ with $\\overline{\\mathbf{x}}$\n",
    "the ensemble mean and $\\mathbf{x}^{(n)}$ the $n$th ensemble member. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd5feb",
   "metadata": {},
   "source": [
    "4. Try different values for `inflation` always in combination with `x0=xguess, n_ens=12`. (hint: `exp = Experiment(x0=xguess, t=t, xt=xt, n_ens=12, inflation=0.0)`)\n",
    "What is the smallest value for which spread and RMSE approximately match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84729e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51f5e19b",
   "metadata": {},
   "source": [
    "5. Repeat 4. but now using `obs_grid='1010'`. What is now the smallest inflation factor for which spread and \n",
    "RMSE match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98c09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e183bbaa",
   "metadata": {},
   "source": [
    "Theoretically the covariance after the DA step (i.e. analysis covariance) should be given as\n",
    "\\begin{equation}\n",
    "    \\mathbf{P}^{a} = \\mathbf{P}^{f} - \\mathbf{P}^{f}\\mathbf{H}^{\\rm{T}}(\\mathbf{R}+\\mathbf{H}\\mathbf{P}^{f}\\mathbf{H}^{\\rm{T}})^{-1}\\mathbf{H} \\mathbf{P}^{f}\n",
    "\\end{equation}    \n",
    "\n",
    "For SEnKF this equality only holds on average, i.e.\n",
    "\\begin{equation}\n",
    "E[ \\frac{1}{N_{ens}-1} \\sum_{n=1}^{N_{ens}} (x^{a,(n)}-\\overline{x^{a}}) \n",
    "(x^{a,(n)}-\\overline{x^{a}})^{\\rm{T}} ] = \\mathbf{P}^{a}\n",
    "\\end{equation}\n",
    "However, single instances of \n",
    "\\begin{equation}\n",
    "\\frac{1}{N_{ens}-1} \\sum_{n=1}^{N_{ens}} (x^{a,(n)}-\\overline{x^{a}}) \n",
    "(x^{a,(n)}-\\overline{x^{a}})^{\\rm{T}}\n",
    "\\end{equation}\n",
    "might differ from $\\mathbf{A}$. \n",
    "\n",
    "The ensemble transform Kalman filter (ETKF) is an alternative to SEnKF in which no perturbations are added to the \n",
    "observations.\n",
    "Instead a linear combination of background ensemble states is sought such that \n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{X}}^{a} = \\mathbf{\\tilde{X}}^{b} \\mathbf{T}\n",
    "\\end{equation}\n",
    "and $\\frac{1}{N_{ens}-1}\\mathbf{\\tilde{X}}^{a} \\mathbf{\\tilde{X}}^{a,\\rm{T}} = \\mathbf{P}^{a}$. \n",
    "Here the $n$th-column of $\\tilde{\\mathbf{X}}$ is $x^{(n)}-\\overline{x}$ the perturbation \n",
    "of the $n$th-ensemble member from the ensemble mean. Rewriting $\\mathbf{P}^{a}$ as \n",
    "\\begin{equation}\n",
    "\\mathbf{P}^{a} \\overset{def}{=} \\mathbf{\\tilde{X}}^{b} \\mathbf{T} \\mathbf{T}^{\\rm{T}} \\mathbf{\\tilde{X}}^{b,\\rm{T}}\n",
    "\\end{equation}\n",
    "gives that the aforementioned equality can be achieved by setting \n",
    "\\begin{equation}\n",
    "\\mathbf{T} = \\mathbf{U} \\mathbf{\\Lambda}^{-1/2} \\mathbf{U}^{\\rm{T}}\n",
    "\\end{equation}\n",
    "with $\\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^{\\rm{T}}$ the singular-value decomposition of \n",
    "$\\mathbf{T} \\mathbf{T}^{\\rm{T}}=\\mathbf{I}-\\mathbf{\\tilde{X}}^{b,\\rm{T}} \\mathbf{H}^{\\rm{T}}(\\mathbf{H}\\mathbf{P}^{f}\\mathbf{H}^{\\rm{T}}+\\mathbf{R})^{-1}\\mathbf{H}\\mathbf{\\tilde{X}}^{b}$. Because it is exact up to the 2nd statistical momentum,\n",
    "we say that ETKF is a 2nd order transform.\n",
    "\n",
    "6. Repeat experiment 4. now using `da_method='ETKF'`. Does ETKF require more or less inflation than SEnKF? (hint: `exp = Experiment(x0=xguess, t=t, xt=xt, n_ens=12, inflation=0.0, da_method='ETKF')`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21bdf04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d05db49d",
   "metadata": {},
   "source": [
    "## Localisation\n",
    "\n",
    "We are now going look into the effects of localisation on the performance of the ensemble Kalman filter. \n",
    "We will focus on model space localisation in the stochastic EnKF. The domain\n",
    "localisation needed for ETKF is very slow without parallelisation, so we are not going to use ETKF in this section.\n",
    "\n",
    "First, we create a class to hold the default settings for the experiments in this section. These defaults are \n",
    "slightly different from the ones in the previous section. Here we will be using the\n",
    "stochastic ensemble Kalman filter with 12 ensemble members to assimilate every 2nd point, every 2nd time step using \n",
    "an observational error standard deviation of $\\sigma_{obs} = \\sqrt{2}$, an inflation factor of 0.1. By \n",
    "default localisation is not yet activated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalisationExperiment(Experiment):\n",
    "    \"\"\" \n",
    "    Class that overwrites the defaults for period_obs, inflation and obs_grid in the Experiment class.     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        defaults = {'period_obs' : 2,\n",
    "                   'inflation' : 0.05,\n",
    "                   'obs_grid' : '1010',\n",
    "                   'n_ens' : 12}\n",
    "        defaults = {**defaults, **kwargs}\n",
    "        super().__init__(**defaults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbd85c",
   "metadata": {},
   "source": [
    "As reference we run the experiment with the default settings and without localisation using the initial guess\n",
    "that deviates from the truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the experiment\n",
    "exp = LocalisationExperiment(x0 = xguess, t=t, xt=xt)\n",
    "exp.run()\n",
    "\n",
    "#Plot state and error metrics. \n",
    "exp.plot_state()\n",
    "exp.plot_metrics(1)\n",
    "\n",
    "#Calculate the root-mean-square values over time. \n",
    "np.sqrt((exp.calculate_metrics(1)**2).mean(dim=['time'])).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921b03d",
   "metadata": {},
   "source": [
    "We see that the RMSE grows considerably larger than the ensemble spread. Here we will \n",
    "test whether localisation can fix this. \n",
    "\n",
    "With domain localisation the stochastic ensemble Kalman filter correction for ensemble member $n$ becomes \n",
    "\\begin{equation}\n",
    "x^{a,(n)} = x^{b,(n)} \n",
    "+ (\\mathbf{L} \\circ \\mathbf{P}^{f}) \\mathbf{H}^{\\rm{T}} (\\mathbf{R}+\\mathbf{H} (\\mathbf{L} \\circ \\mathbf{P}^{f})\\mathbf{H}^{\\rm{T}})^{-1}\n",
    "(y - \\mathbf{H} x^{b,(n)} + \\mathbf{R}^{\\frac{1}{2}}\\epsilon^{(n)})\n",
    "\\end{equation}\n",
    "with $\\circ$ the Hadamard, or elementwise, product and $\\mathbf{L}$ the localisation matrix.  \n",
    "\n",
    "To illustrate the effect of ensemble size and localisation we first run the experiment with a large ensemble \n",
    "of 256 members and plot the intial background error covariance together with its eigenvalues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65076948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the experiment\n",
    "exp = LocalisationExperiment(x0 = xguess, t=t, xt=xt, n_ens=256)\n",
    "exp.run()\n",
    "\n",
    "#Plot the covariance at t=0 together with its eigenvalues. \n",
    "P = np.cov(exp.Xb[:,:,0])\n",
    "plot_LocMatrix(P, name='true covariance')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba6988",
   "metadata": {},
   "source": [
    "7. Plot the initial background covariance, but now from an ensemble with `n_ens=12` members. Is the background error\n",
    "covariance and its spectrum the same as the one obtained from 256-member ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a513fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9563c4ac",
   "metadata": {},
   "source": [
    "In order for our 12-member ensemble covariance to give a closer approximation to the true covariance,\n",
    "we are going to apply localisation. The localisation weights every element in the background error covariance matrix. Elements on the diagonal are multiplied\n",
    "by 1, while those off the diagonal are multiplied with values $<1$ that decrease as the elements are located further\n",
    "from the diagonal. I.e. we are going to look at $\\mathbf{L} \\circ \\mathbf{P}^{f}$, the domain localisation \n",
    "case in which the observation operator $\\mathbf{H}$ is the identity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78632dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the experiment\n",
    "exp = LocalisationExperiment(x0 = xguess, t=t, xt=xt, loc_radius=2.0)\n",
    "exp.run()\n",
    "\n",
    "#Plot the localisation matrix. \n",
    "plot_LocMatrix(np.array(exp.L_x), name='localisation')\n",
    "\n",
    "#Localise background error matrix by taking the Hadamard product with the localisation matrix. \n",
    "P = np.cov(exp.Xb[:,:,0]) * exp.L_x\n",
    "\n",
    "#Plot the localised background error covariance matrix. \n",
    "plot_LocMatrix(P, name='localised covariance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a12db1",
   "metadata": {},
   "source": [
    "8. Compare the localised covariance with the one from the 256-member ensemble. What is the effect of localisation on the\n",
    "the covariance matrix and what is the effect on its spectrum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58af320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dc11154",
   "metadata": {},
   "source": [
    "9. Run the using settings `x0 = xguess, loc_radius=2.0` like in the previous experiment. \n",
    "Plot the RMSE using `exp.plot_metrics()`. Does the application of localisation reduce the RMSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97b0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06651a43",
   "metadata": {},
   "source": [
    "10. Repeat experiment 9 `for loc_radius in [None, 1.0, 2.0, 5.0, 9.0]`. Which localisation radius gives you the \n",
    "lowest RMSE? At which `loc_radius` do you get the lowest RMSE if you also pass `n_ens=24` as argument to \n",
    "`LocalisationExperiment`? What do you conclude about the nature of the (qualitative) relation between \n",
    "the optimal localisation radius and the ensemble size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ceb4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53bf46e8",
   "metadata": {},
   "source": [
    "11. In the previous section, we explained that the RMSE and ensemble spread should, on average,\n",
    "be equal. We also showed that ensemble inflation can be used to achieve this. For `n_ens=12` and \n",
    "`for loc_radius in [None, 2, 5]` find the smallest nonnegative value of `inflation` for which RMSE and \n",
    "ensemble spread are approximately equal over the simulation period. How does the localisation radius \n",
    "impact the need for inflation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c462b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2837e00",
   "metadata": {},
   "source": [
    "12. Time permitting repeat experiment 10 using `obs_grid = 'landsea'`. How does the design of the observation network impact\n",
    "the need for localisation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd83fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
